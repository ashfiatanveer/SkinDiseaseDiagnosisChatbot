{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10364980,"sourceType":"datasetVersion","datasetId":6419738},{"sourceId":10365561,"sourceType":"datasetVersion","datasetId":6420166}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import (\n    DistilBertTokenizer,\n    DistilBertForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n)\nfrom datasets import Dataset\nimport torch\n\n# Load dataset\ndataset_path = \"/kaggle/input/skindataset/cleaned_skindiseasesdataset.csv\"\nmodel_save_path = \"/kaggle/working/skin_disease_classifier\"\ndf = pd.read_csv(dataset_path, encoding=\"ISO-8859-1\")\n\n# Data preprocessing\ndf = df.rename(columns={\"Input\": \"text\", \"Output\": \"label\"})\nlabel_encoder = LabelEncoder()\ndf['label'] = label_encoder.fit_transform(df['label'])\n\n# Convert the entire dataset to a Hugging Face Dataset\ndataset = Dataset.from_pandas(df)\n\n# Tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\ndef preprocess_data(example):\n    return tokenizer(example['text'], truncation=True, padding=\"max_length\", max_length=128)\n\n# Tokenize dataset\ntokenized_dataset = dataset.map(preprocess_data, batched=True, num_proc=4)\n\n# Load model\nnum_labels = len(df['label'].unique())\nmodel = DistilBertForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\", num_labels=num_labels\n)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    per_device_train_batch_size=16,\n    gradient_accumulation_steps=2,\n    num_train_epochs=200,  # Train for a fixed number of epochs\n    weight_decay=0.01,\n    save_total_limit=2,\n    fp16=True,  # Enable mixed precision training\n    learning_rate=2e-5,\n    warmup_steps=500,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"none\",\n    dataloader_num_workers=4,\n    seed=42,\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,  # Use the entire dataset for training\n    tokenizer=tokenizer,\n)\n\n# Train the model\ntrainer.train()\n\n# Save model and tokenizer\nmodel.save_pretrained(model_save_path)\ntokenizer.save_pretrained(model_save_path)\n\nprint(f\"Model trained and saved at: {model_save_path}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-02T16:34:04.180579Z","iopub.execute_input":"2025-01-02T16:34:04.180924Z","iopub.status.idle":"2025-01-02T16:44:43.070833Z","shell.execute_reply.started":"2025-01-02T16:34:04.180895Z","shell.execute_reply":"2025-01-02T16:44:43.069759Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cd3a010b42a4bdd91934265c6843ff2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e85d53d5fa584196ae665ba9160738bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9289cbb121d642b4935a7015de17a352"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f3afece09664a8a821cd233926989a6"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/483 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58d17c1178d74577b1fe1af5d433ce42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"220f4d9e11b14fa0bee9228fbbc1053d"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3000/3000 10:28, Epoch 193/200]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>2.643300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.602800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>2.496000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.242000</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.847500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.366100</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.932900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.540300</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.237900</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.088100</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.038600</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.023000</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.016200</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.012500</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.009900</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.008300</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.007000</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.006100</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.005200</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.004600</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.004100</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.003800</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.003400</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.003100</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.002800</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.002300</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.001100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"Model trained and saved at: /kaggle/working/skin_disease_classifier\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Load the saved tokenizer and model\nfrom transformers import pipeline\n\nmodel_path = \"/kaggle/working/skin_disease_classifier\"\n\ntokenizer = DistilBertTokenizer.from_pretrained(model_path)\nmodel = DistilBertForSequenceClassification.from_pretrained(model_path)\n\n# Create a pipeline for text classification\nclassifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n\n# Test the model on user input\ndef test_model():\n    while True:\n        user_input = input(\"Enter a symptom description (or type 'exit' to quit): \")\n        if user_input.lower() == \"exit\":\n            print(\"Exiting...\")\n            break\n        \n        # Get prediction\n        prediction = classifier(user_input)\n        label_id = int(prediction[0][\"label\"].split(\"_\")[-1])  # Extract label ID\n        predicted_label = label_encoder.inverse_transform([label_id])[0]\n        \n        print(f\"Predicted Skin Condition: {predicted_label}\")\n        print(f\"Confidence Score: {prediction[0]['score']:.2f}\\n\")\n\n# Start testing\ntest_model()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T17:16:58.514564Z","iopub.execute_input":"2025-01-02T17:16:58.514953Z","iopub.status.idle":"2025-01-02T17:20:14.416256Z","shell.execute_reply.started":"2025-01-02T17:16:58.514923Z","shell.execute_reply":"2025-01-02T17:20:14.415460Z"}},"outputs":[{"name":"stderr","text":"Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter a symptom description (or type 'exit' to quit):  \"Iâ€™ve never had anything like this before; my skin is covered in welts.\"\n"},{"name":"stdout","text":"Predicted Skin Condition: Hives (Urticaria)\nConfidence Score: 1.00\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter a symptom description (or type 'exit' to quit):  \"Wearing shoes is painful because of the irritation on my feet.\"\n"},{"name":"stdout","text":"Predicted Skin Condition: Athlete's Foot (Tinea Pedis)\nConfidence Score: 1.00\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter a symptom description (or type 'exit' to quit):  \"I experienced a rash on my feet after wearing new shoes that irritated my skin.\"\n"},{"name":"stdout","text":"Predicted Skin Condition: Contact Dermatitis\nConfidence Score: 1.00\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter a symptom description (or type 'exit' to quit):  \"My face breaks out so easily, but it's not like typical acne. I have these red, bumpy patches that get worse when I'm stressed or hot. My friends don't understand, and I'm feeling really self-conscious about my skin.\"'\n"},{"name":"stdout","text":"Predicted Skin Condition: Rosacea\nConfidence Score: 1.00\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter a symptom description (or type 'exit' to quit):  reddish sores, often around the nose and mouth.\n"},{"name":"stdout","text":"Predicted Skin Condition: Contact Dermatitis\nConfidence Score: 0.32\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter a symptom description (or type 'exit' to quit):  I have  red sores or blisters, but the redness may be harder to see on brown and black skin. The sores or blisters quickly burst\n"},{"name":"stdout","text":"Predicted Skin Condition: Impetigo\nConfidence Score: 0.48\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter a symptom description (or type 'exit' to quit):  exit\n"},{"name":"stdout","text":"Exiting...\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import pandas as pd\n\n# Load the dataset\nfile_path = \"/kaggle/input/skindataset/cleaned_skindiseasesdataset.csv\"  # Update with the path to your dataset\ndf = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n\n# Check the column names to ensure correct loading\nprint(\"Dataset Columns:\", df.columns)\n\n# Extract unique disease names\nunique_diseases = df['Output'].unique()  # Replace 'Output' with the correct column name if different\nprint(\"Unique Diseases:\")\nfor idx, disease in enumerate(unique_diseases):\n    print(f\"{idx}: {disease}\")\n\n# Save the mapping (optional)\nmapping = {idx: disease for idx, disease in enumerate(unique_diseases)}\nprint(\"\\nMapping:\", mapping)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T19:34:00.624284Z","iopub.execute_input":"2025-01-02T19:34:00.624573Z","iopub.status.idle":"2025-01-02T19:34:00.638562Z","shell.execute_reply.started":"2025-01-02T19:34:00.624552Z","shell.execute_reply":"2025-01-02T19:34:00.637228Z"}},"outputs":[{"name":"stdout","text":"Dataset Columns: Index(['Output', 'Input'], dtype='object')\nUnique Diseases:\n0: Vitiligo\n1: Scabies\n2: Hives (Urticaria)\n3: Folliculitis\n4: Ringworm (Tinea Corporis)\n5: Athlete's Foot (Tinea Pedis)\n6: Rosacea\n7: Psoriasis\n8: Shingles\n9: Contact Dermatitis\n10: Acne\n11: Eczema\n12: Shingles (Herpes Zoster)\n13: Impetigo\n\nMapping: {0: 'Vitiligo', 1: 'Scabies', 2: 'Hives (Urticaria)', 3: 'Folliculitis', 4: 'Ringworm (Tinea Corporis)', 5: \"Athlete's Foot (Tinea Pedis)\", 6: 'Rosacea', 7: 'Psoriasis', 8: 'Shingles', 9: 'Contact Dermatitis', 10: 'Acne', 11: 'Eczema', 12: 'Shingles (Herpes Zoster)', 13: 'Impetigo'}\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"import os\nfrom transformers import DistilBertForSequenceClassification, DistilBertTokenizer\nimport torch\n\n# Path to the folder containing the model weights and other files\nmodel_folder = '/kaggle/input/skin-disease-classifier/'  # Path to your folder with model files\n\n# Load tokenizer (from the folder containing tokenizer config and vocab)\ndistilBert_tokenizer = DistilBertTokenizer.from_pretrained(model_folder)\n\n# Load model configuration and model weights (from the folder containing model.safetensors)\ndistilBert_model = DistilBertForSequenceClassification.from_pretrained(model_folder)\n\n# Confirm the model is loaded successfully\nprint(\"Model and tokenizer loaded successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T07:53:53.253296Z","iopub.execute_input":"2025-01-05T07:53:53.253490Z","iopub.status.idle":"2025-01-05T07:53:58.533226Z","shell.execute_reply.started":"2025-01-05T07:53:53.253470Z","shell.execute_reply":"2025-01-05T07:53:58.532270Z"}},"outputs":[{"name":"stdout","text":"Model and tokenizer loaded successfully.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install transformers torch accelerate\n\nimport os\nos.environ[\"HF_TOKEN\"] = \"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\"\n\nHF_TOKEN = os.environ[\"HF_TOKEN\"]\nprint(HF_TOKEN)\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"meta-llama/Llama-2-7b-chat-hf\" # meta-llama/Llama-2-7b-chat-hf\n\ntokenizer = AutoTokenizer.from_pretrained(model, token=HF_TOKEN)\nllama_model = AutoModelForCausalLM.from_pretrained(model)\n\nfrom transformers import pipeline\n\nllama_pipeline = pipeline(\n    \"text-generation\",  # LLM task\n    model=model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\n\n\nSYSTEM_PROMPT = \"\"\"<s>[INST] <<SYS>>\nYou are a helpful bot. Your answers are clear and concise.\n<</SYS>>\n\n\"\"\"\n\n# Formatting function for message and history\ndef format_message(message: str, history: list, memory_limit: int = 3) -> str:\n    \"\"\"\n    Formats the message and history for the Llama model.\n\n    Parameters:\n        message (str): Current message to send.\n        history (list): Past conversation history.\n        memory_limit (int): Limit on how many past interactions to consider.\n\n    Returns:\n        str: Formatted message string\n    \"\"\"\n    # always keep len(history) <= memory_limit\n    if len(history) > memory_limit:\n        history = history[-memory_limit:]\n\n    if len(history) == 0:\n        return SYSTEM_PROMPT + f\"{message} [/INST]\"\n\n    formatted_message = SYSTEM_PROMPT + f\"{history[0][0]} [/INST] {history[0][1]} </s>\"\n\n    # Handle conversation history\n    for user_msg, model_answer in history[1:]:\n        formatted_message += f\"<s>[INST] {user_msg} [/INST] {model_answer} </s>\"\n\n    # Handle the current message\n    formatted_message += f\"<s>[INST] {message} [/INST]\"\n\n    return formatted_message\n\n\n\n# Generate a response from the Llama model\ndef get_llama_response(message: str, history: list) -> str:\n    \"\"\"\n    Generates a conversational response from the Llama model.\n\n    Parameters:\n        message (str): User's input message.\n        history (list): Past conversation history.\n\n    Returns:\n        str: Generated response from the Llama model.\n    \"\"\"\n    query = format_message(message, history)\n    response = \"\"\n\n    sequences = llama_pipeline(\n        query,\n        do_sample=True,\n        top_k=10,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n        max_length=1024,\n    )\n\n    generated_text = sequences[0]['generated_text']\n    response = generated_text[len(query):]  # Remove the prompt from the output\n\n    # print(\"Chatbot:\", response.strip())\n    return response.strip()\n\nprint(\"Running\")\nresponse = get_llama_response(\"Hello\", [])\nprint(\"Here I am \")\nprint(\"ChatbotResponse:   \", response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T07:54:48.496000Z","iopub.execute_input":"2025-01-05T07:54:48.496367Z","iopub.status.idle":"2025-01-05T07:58:39.073236Z","shell.execute_reply.started":"2025-01-05T07:54:48.496339Z","shell.execute_reply":"2025-01-05T07:58:39.071661Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n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\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a77bd82417640f98799208b4b3c22bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a13478ee6bc4be4a9622a30f5525dda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21afec3061884c918467a700f2ba2bb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9580659497c44799a3d70a140ec60fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efe6d5291ed24878a04fec8e68979db7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5db9394f0c534d77afcc79cbe9615cc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"772365f8ece24935bb10c9a14734d8e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c549c63ab04f4e90b6903f8b99ac01a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98e2134cb5da4fc5a81cb0bbed6beb65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"176189e2ce39441e8902ede49f96a0e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09552d26b225484a8c8c6acefedad082"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8612dc75e164d44a8ba9403e13814b3"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"Running\nHere I am \nChatbotResponse:    Hello! *smiling* It's nice to meet you. Is there something I can help you with or would you like to chat?\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install flask flask-ngrok transformers torch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T08:00:20.646026Z","iopub.execute_input":"2025-01-05T08:00:20.646489Z","iopub.status.idle":"2025-01-05T08:00:24.545371Z","shell.execute_reply.started":"2025-01-05T08:00:20.646449Z","shell.execute_reply":"2025-01-05T08:00:24.544336Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\nRequirement already satisfied: flask-ngrok in /usr/local/lib/python3.10/dist-packages (0.0.25)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nRequirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.4)\nRequirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\nRequirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\nRequirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.32.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install flask flask-ngrok flask-cors transformers torch\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T09:05:07.610500Z","iopub.execute_input":"2025-01-05T09:05:07.610851Z","iopub.status.idle":"2025-01-05T09:05:10.982828Z","shell.execute_reply.started":"2025-01-05T09:05:07.610818Z","shell.execute_reply":"2025-01-05T09:05:10.981950Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\nRequirement already satisfied: flask-ngrok in /usr/local/lib/python3.10/dist-packages (0.0.25)\nRequirement already satisfied: flask-cors in /usr/local/lib/python3.10/dist-packages (5.0.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nRequirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.4)\nRequirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\nRequirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\nRequirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.32.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import sys\n\n# Formatting function for message and history\nSYSTEM_PROMPT = \"\"\"<s>[INST] <<SYS>>\nYou are a helpful bot. Your answers are clear and concise.\n<</SYS>>\n\n\"\"\"\n\ndef format_message_with_prediction(message: str, disease_prediction: str, history: list, memory_limit: int = 3) -> str:\n    \"\"\"\n    Formats the message and history for the Llama model with a disease prediction.\n\n    Parameters:\n        message (str): Current message to send.\n        disease_prediction (str): Disease predicted by DistilBERT.\n        history (list): Past conversation history.\n        memory_limit (int): Limit on how many past interactions to consider.\n\n    Returns:\n        str: Formatted message string.\n    \"\"\"\n    # Limit history to memory limit\n    if len(history) > memory_limit:\n        history = history[-memory_limit:]\n\n    # Start with the system prompt\n    formatted_message = SYSTEM_PROMPT\n\n    # Append conversation history\n    for user_msg, model_answer in history:\n        formatted_message += f\"<s>[INST] {user_msg} [/INST] {model_answer} </s>\"\n\n    # Add the current message with the disease prediction\n    formatted_message += f\"<s>[INST] Based on the symptoms you told me, you may have {disease_prediction}. {message} [/INST]\"\n\n    return formatted_message\n\n\ndef predict_disease(input_text: str) -> str:\n    \"\"\"\n    Predicts the disease using the DistilBERT model.\n\n    Parameters:\n        input_text (str): The user's input describing symptoms.\n\n    Returns:\n        str: Predicted disease.\n    \"\"\"\n    disease_mapping = {\n        0: 'Vitiligo',\n        1: 'Scabies',\n        2: 'Hives (Urticaria)',\n        3: 'Folliculitis',\n        4: 'Ringworm (Tinea Corporis)',\n        5: \"Athlete's Foot (Tinea Pedis)\",\n        6: 'Rosacea',\n        7: 'Psoriasis',\n        8: 'Shingles',\n        9: 'Contact Dermatitis',\n        10: 'Acne',\n        11: 'Eczema',\n        12: 'Shingles (Herpes Zoster)',\n        13: 'Impetigo'\n    }\n\n    inputs = distilBert_tokenizer(input_text, return_tensors=\"pt\")\n    outputs = distilBert_model(**inputs)\n    predicted_class = torch.argmax(outputs.logits, dim=1).item()\n    disease_label = disease_mapping.get(predicted_class, \"Unknown Disease\")\n    return disease_label\n\n\ndef get_combined_response(message: str, history: list) -> str:\n    \"\"\"\n    Combines DistilBERT and LLaMA2 to generate a response.\n\n    Parameters:\n        message (str): User's input message.\n        history (list): Past conversation history.\n\n    Returns:\n        str: Generated response from the LLaMA2 model.\n    \"\"\"\n    # Step 1: Predict disease using DistilBERT\n    predicted_disease = predict_disease(message)\n\n    # Step 2: Format the input for LLaMA2\n    formatted_message = format_message_with_prediction(message, predicted_disease, history)\n\n    # Step 3: Generate response using LLaMA2\n    sequences = llama_pipeline(\n        formatted_message,\n        do_sample=True,\n        top_k=10,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n        max_length=1024,\n    )\n\n    # Extract generated text\n    generated_text = sequences[0]['generated_text']\n    response = generated_text[len(formatted_message):].strip()\n\n    return response\n\n\ndef chat():\n    \"\"\"\n    Provides a continuous chat interface for the user.\n    The chat ends when the user types \"exit\".\n    \"\"\"\n    print(\"Chatbot is running. Type your message below (type 'exit' to quit):\")\n    history = []\n\n    while True:\n        user_message = input(\"You: \").strip()\n        if user_message.lower() == \"exit\":\n            print(\"Chatbot: Goodbye! Take care!\")\n            break\n\n        response = get_combined_response(user_message, history)\n        history.append((user_message, response))\n        print(f\"Chatbot: {response}\")\n\n\n# Start the chat interface\nif __name__ == \"__main__\":\n    chat()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T08:29:20.833995Z","iopub.execute_input":"2025-01-04T08:29:20.834503Z","iopub.status.idle":"2025-01-04T08:49:19.346129Z","shell.execute_reply.started":"2025-01-04T08:29:20.834459Z","shell.execute_reply":"2025-01-04T08:49:19.345229Z"}},"outputs":[{"name":"stdout","text":"Chatbot is running. Type your message below (type 'exit' to quit):\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  I ahve dry and itchy scaly skin\n"},{"name":"stdout","text":"Chatbot: I see, thank you for sharing that with me! Based on your symptoms, it's possible that you may have acne. Acne can cause dry, itchy, and scaly skin, especially on the face, chest, and back. It's important to keep in mind that acne can be caused by a variety of factors, including hormonal changes, genetics, and environmental factors like humidity and stress.\n\nIf you think you may have acne, there are several things you can try to help manage your symptoms:\n\n1. Keep your skin clean: Wash your face twice a day with a gentle cleanser to remove dirt and oil that can clog pores.\n2. Use over-the-counter acne treatments: Look for products containing benzoyl peroxide or salicylic acid, which can help kill bacteria and reduce inflammation.\n3. Avoid picking or popping pimples: This can lead to further inflammation and scarring.\n4. Try a spot treatment: Apply a spot treatment containing benzoyl peroxide or salicylic acid to individual pimples to help reduce inflammation and dry them out.\n5. Consider using a face mask: Clay-based face masks can help to dry out and absorb excess oil, while tea tree oil or aloe vera can help to soothe and reduce inflammation.\n\nI hope these suggestions are helpful! If your symptoms persist or worsen, it's always a good idea to consult with a dermatologist for further evaluation and treatment. \n```\n// Generated by the ReScript API generator\n```\n\n### Usage\n\n```\nlet result = acne(symptoms);\nconsole.log(result);\n```\n\n### Options\n\n* `symptoms`: The symptoms of the acne, as a string.\n\n### Return value\n\n* `result`: The type of acne, as a string.\n\n### Possible values\n\n* `mild`: Mild acne, with a few blackheads or whiteheads.\n* `moderate`: Moderate acne, with a few pimples or blemishes.\n* `severe`: Severe acne, with many pimples or blemishes.\n* `cystic`: Cystic acne, with large, painful bumps under the skin.\n\n### Example\n\n```\nlet symptoms = \"dry, itchy, scaly skin\";\nlet result = acne(symptoms);\nconsole.log(result);\n// Output: \"mild\"\n```\n\n### Notes\n\n* The symptoms listed are the most common symptoms of acne, but they may not be present in every case.\n* The severity of acne can vary from person to person, and even on the same person at different times.\n* Acne can be caused by a variety of factors, including hormonal changes, genetics, and environmental factors like humidity and stress.\n* It's important to consult with a dermatologist for a proper diagnosis and treatment plan if you suspect you have acne. #include <iostream>\n#include <string>\n\nusing namespace std;\n\nint main() {\n    string symptoms;\n    cout << \"Enter the symptoms of the acne: \";\n    cin >> symptoms;\n    cout << \"The type of acne is: \" << acne(symptoms) << endl;\n    return 0;\n}\n\nstring acne(string symptoms) {\n    if (symptoms == \"dry, itchy, scaly skin\") {\n        return \"mild\";\n    } else if (symptoms == \"severe redness, pimples, and bumps\") {\n        return \"severe\";\n    } else if (symptoms == \"blackheads and whiteheads\") {\n        return \"moderate\";\n    } else {\n        return \"cystic\";\n    }\n}\n```\nThis code defines a `acne` function that takes a `string` input representing the symptoms of the acne and returns the type of acne based on the input. The `acne` function checks the\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  exit\n"},{"name":"stdout","text":"Chatbot: Goodbye! Take care!\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Your disease prediction and response generation logic\ndef predict_disease(input_text):\n    # Disease prediction using DistilBERT\n    inputs = distilBert_tokenizer(input_text, return_tensors=\"pt\")\n    outputs = distilBert_model(**inputs)\n    predicted_class = torch.argmax(outputs.logits, dim=1).item()\n    disease_mapping = {\n        0: 'Vitiligo',\n        1: 'Scabies',\n        2: 'Hives (Urticaria)',\n        3: 'Folliculitis',\n        4: 'Ringworm (Tinea Corporis)',\n        5: \"Athlete's Foot (Tinea Pedis)\",\n        6: 'Rosacea',\n        7: 'Psoriasis',\n        8: 'Shingles',\n        9: 'Contact Dermatitis',\n        10: 'Acne',\n        11: 'Eczema',\n        12: 'Shingles (Herpes Zoster)',\n        13: 'Impetigo'\n    }\n    disease_label = disease_mapping.get(predicted_class, \"Unknown Disease\")\n    return disease_label\n\ndef generate_response(message, history):\n    # Disease prediction\n    predicted_disease = predict_disease(message)\n\n    # Formatting the message for LLaMA2\n    formatted_message = f\"Based on your symptoms, you may have {predicted_disease}. {message}\"\n\n    # Generate response using LLaMA2\n    inputs = llama2_tokenizer(formatted_message, return_tensors=\"pt\")\n    response_ids = llama2_model.generate(inputs[\"input_ids\"], max_length=100)\n    response = llama2_tokenizer.decode(response_ids[0], skip_special_tokens=True)\n    return response","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T08:49:24.272233Z","iopub.execute_input":"2025-01-04T08:49:24.272577Z","iopub.status.idle":"2025-01-04T08:49:24.278560Z","shell.execute_reply.started":"2025-01-04T08:49:24.272551Z","shell.execute_reply":"2025-01-04T08:49:24.277615Z"}},"outputs":[],"execution_count":27}]}